{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sreel\\documents\\project\\karate project2.0\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import pandas as pd\n",
    "\n",
    "landmarks2 = []\n",
    "count = 0\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate face direction\n",
    "def calculate_face_direction(nose, left_eye, right_eye, left_ear, right_ear):\n",
    "    # Calculate midpoint of eyes\n",
    "    eyes_midpoint = ((left_eye[0] + right_eye[0]) / 2, (left_eye[1] + right_eye[1]) / 2)\n",
    "    \n",
    "    # Calculate eye distance to set tolerance\n",
    "    eye_distance = abs(right_eye[0] - left_eye[0])\n",
    "    tolerance = eye_distance * 0.5  # 10% of eye distance as tolerance\n",
    "    \n",
    "    # Calculate horizontal direction (yaw)\n",
    "    nose_offset = nose[0] - eyes_midpoint[0]\n",
    "    \n",
    "    # Use tolerance zone for forward position\n",
    "    if abs(nose_offset) <= tolerance:\n",
    "        yaw = \"forward\"\n",
    "    elif nose_offset < -tolerance:\n",
    "        yaw = \"left\"\n",
    "    else:\n",
    "        yaw = \"right\"\n",
    "\n",
    "    # Calculate vertical direction (pitch)\n",
    "    if nose[1] < eyes_midpoint[1]:\n",
    "        pitch = \"up\"\n",
    "    elif nose[1] > eyes_midpoint[1]:\n",
    "        pitch = \"down\"\n",
    "    else:\n",
    "        pitch = \"forward\"\n",
    "\n",
    "    return yaw, pitch\n",
    " \n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    # Initialize video capture and writer\n",
    "    global landmarks2\n",
    "    landmarks2 = []  # Reset landmarks list\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {input_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process frames with progress bar\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        frame_number += 1  # Increment frame counter\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pose_results = pose.process(rgb_frame)\n",
    "        \n",
    "        if pose_results.pose_landmarks:\n",
    "            landmarks = pose_results.pose_landmarks.landmark\n",
    "            nose = (landmarks[mp_pose.PoseLandmark.NOSE.value].x, landmarks[mp_pose.PoseLandmark.NOSE.value].y)\n",
    "            left_eye = (landmarks[mp_pose.PoseLandmark.LEFT_EYE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_EYE.value].y)\n",
    "            right_eye = (landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value].y)\n",
    "            left_ear = (landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].x, landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].y)\n",
    "            right_ear = (landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].y)\n",
    "            \n",
    "            yaw, pitch = calculate_face_direction(nose, left_eye, right_eye, left_ear, right_ear)\n",
    "            if yaw != \"forward\":\n",
    "                extracted_landmarks = [(lm.x, lm.y, lm.z, lm.visibility) for lm in landmarks]  # Extract numerical data\n",
    "                landmarks2.append([extracted_landmarks, frame_number])  # Appending two columns\n",
    "\n",
    "            # if(yaw!=\"forward\"):\n",
    "            #     print(landmarks)\n",
    "                \n",
    "            #     landmarks2.append([landmarks, frame_number])  # Each frame gets its own entry\n",
    "            #     out.write(frame)\n",
    "        \n",
    "        \n",
    "        # Display frame (press 'q' to quit early)\n",
    "        #cv2.imshow('Processing', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return landmarks2\n",
    "\n",
    "# Usage\n",
    "input_video = \"trimmedVideo/frontView-1.mp4\"  # Replace with your video path\n",
    "output_video = \"frontView-Leftright.mp4\"\n",
    "landmarks = process_video(input_video, output_video)\n",
    "#print(landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define column names (including frame_number)\n",
    "columns = ['frame_number']\n",
    "for i in range(33):  # 33 landmarks (0 to 32)\n",
    "    columns.extend([f'{i}_x', f'{i}_y', f'{i}_z', f'{i}_visibility'])\n",
    "\n",
    "# Convert landmarks2 list to a DataFrame\n",
    "structured_data = []\n",
    "for landmarks, frame_number in landmarks:\n",
    "    # Flatten landmark data into a single row\n",
    "    row = [frame_number]  # Start with frame number\n",
    "    for lm in landmarks:  # lm is (x, y, z, visibility)\n",
    "        row.extend(lm)\n",
    "    structured_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "landmarks_df = pd.DataFrame(structured_data, columns=columns)\n",
    "\n",
    "landmarks_df.to_csv(\"dataset/1-frontViewTrimmed_frameNo.csv\", index=False)\n",
    "\n",
    "print(f\"CSV file saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
